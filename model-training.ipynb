{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ac9a98-c374-4c30-b5a3-03bf31a9be78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "\n",
    "class analytic_solution:\n",
    "    def __init__(self, ra, r, lambd, mu, sigma):\n",
    "        self.a = (1-ra)/ra * sigma**2\n",
    "        self.b = 2 / ra * (ra * r - r - lambd)\n",
    "        self.c = (lambd + r) ** 2 / (ra * sigma**2)\n",
    "        self.g = -lambd*mu*(lambd + r) / (ra * sigma**2)\n",
    "        self.nu = (self.b ** 2 - 4 * self.a * self.c) ** 0.5\n",
    "        self.d = lambd*mu/ra\n",
    "        self.r = r\n",
    "        self.ra = ra\n",
    "        self.lambd = lambd\n",
    "        self.sigma = sigma\n",
    "        self.mu = mu\n",
    "\n",
    "    def integrand(self, t):\n",
    "        a,d,r,ra,sigma,mu,lambd = self.a, self.d, self.r, self.ra, self.sigma, self.mu, self.lambd\n",
    "        term1 = (a / 2) * (self.B(t) ** 2)\n",
    "        term2 = d * self.B(t)\n",
    "        term3 = (sigma ** 2 / 2) * self.C(t)\n",
    "        term4 = ((lambd*mu) ** 2) / (2 * ra * sigma ** 2)\n",
    "        term5 = r\n",
    "        return term1 + term2 + term3 + term4 + term5\n",
    "\n",
    "\n",
    "    def A(self, tau):\n",
    "        A, _ = quad(self.integrand, 0, tau)\n",
    "        return A\n",
    "\n",
    "    def B(self, tau):\n",
    "        b,nu,g,r = self.b, self.nu, self.g, self.r\n",
    "        return (-4*g*r * (1-np.exp(-nu*tau/2))**2 +2*g*nu*(1-np.exp(-nu*tau))) / (nu * (2*nu - (b+nu)*(1-np.exp(-nu*tau))))\n",
    "\n",
    "\n",
    "    def C(self, tau):\n",
    "        b,c,nu = self.b, self.c, self.nu\n",
    "        return (2*c*(1-np.exp(-nu*tau)))/(2*nu-(b+nu)*(1-np.exp(-nu*tau)))\n",
    "\n",
    "    def optimal_nocost(self, tau, X):\n",
    "        ra,lambd,mu,r,sigma = self.ra, self.lambd, self.mu, self.r, self.sigma\n",
    "        return (1/ra) * ((lambd*(mu-X)/X)-r)/((sigma/X)**2) + (1-ra)/ra * (self.C(tau)*X + self.B(tau))*X\n",
    "\n",
    "    def J(self, W, X, tau):\n",
    "        ra = self.ra\n",
    "        return ((W * (np.exp(self.A(tau) + self.B(tau) * X + (self.C(tau) * (X**2)) / 2)))**(1 - ra) - 1)/(1 - ra)\n",
    "\n",
    "class BoundaryNetwork(nn.Module):\n",
    "\n",
    "    # Define the neural network architecture -- think is correct, may need to change neuron count\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
    "        super(BoundaryNetwork, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim * 4),\n",
    "            nn.LeakyReLU(0.05),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim * 4, output_dim)\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                init.xavier_uniform_(layer.weight, gain=0.01)  # Small gain to reduce initial output magnitude\n",
    "                init.constant_(layer.bias, 0.0)  # Set biases to zero\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Simulate an Ornstein-Uhlenbeck process -- think is correct\n",
    "def simulate_ou_process(x0, mu, sigma, lam, dt, num_steps, num_paths):\n",
    "    x = np.zeros((num_steps + 1, num_paths))\n",
    "    x[0] = x0\n",
    "    for t in range(1, num_steps + 1):\n",
    "        dx = lam * (mu - x[t - 1]) * dt + sigma * np.sqrt(dt) * np.random.randn(num_paths)\n",
    "        x[t] = x[t - 1] + dx\n",
    "    return x\n",
    "\n",
    "# Define the CRRA utility function -- think is correct\n",
    "def crra_utility(w, gamma=2):\n",
    "    small_number = 1\n",
    "    w[w == 0] = small_number  # set 0s to small numbers to avoid issue\n",
    "    if gamma == 1:\n",
    "        return torch.log(w)\n",
    "    else:\n",
    "        return (w ** (1 - gamma) - 1) / (1 - gamma)\n",
    "\n",
    "# Define the loss function -- think is correct\n",
    "def loss_fn(terminal_wealth, starting_wealth, start_price, T, model):\n",
    "    # print(\"Terminal wealth:\", terminal_wealth, 'dimensions:', terminal_wealth.shape)\n",
    "    # print(\"J(w,x,t):\", model.J(starting_wealth, start_price, T), \"U(w):\", crra_utility(terminal_wealth))\n",
    "    return torch.mean(model.J(starting_wealth, start_price, T) - crra_utility(terminal_wealth))\n",
    "\n",
    "def update_wealth(W, pi_new, pi_old, price_change, beta):\n",
    "    transaction_costs = beta * (pi_new - pi_old) ** (3/2)\n",
    "    W_new = W + pi_new * price_change - transaction_costs\n",
    "    return W_new\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ea298-bce5-4f16-8ece-69194fcc155d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/br4250/.conda/envs/torch-env/lib/python3.12/site-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647378361/work/torch/csrc/tensor/python_tensor.cpp:432.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/3000], Loss: 0.0000415870, LR: 0.001000, Average Wealth: 27032.31640625, Average total costs paid: 7860.3037109375\n",
      "Epoch [100/3000], Loss: 0.0000416864, LR: 0.001000, Average Wealth: 27017.95703125, Average total costs paid: 7832.96044921875\n",
      "Epoch [200/3000], Loss: 0.0000416854, LR: 0.001000, Average Wealth: 26999.25, Average total costs paid: 7834.14208984375\n",
      "Epoch [300/3000], Loss: 0.0000416788, LR: 0.001000, Average Wealth: 27026.24609375, Average total costs paid: 7831.857421875\n",
      "Epoch [400/3000], Loss: 0.0000415371, LR: 0.001000, Average Wealth: 27021.94140625, Average total costs paid: 7844.33740234375\n",
      "Epoch [500/3000], Loss: 0.0000417520, LR: 0.000500, Average Wealth: 26933.458984375, Average total costs paid: 7822.26611328125\n",
      "Epoch [600/3000], Loss: 0.0000416579, LR: 0.000500, Average Wealth: 27007.654296875, Average total costs paid: 7831.490234375\n",
      "Epoch [700/3000], Loss: 0.0000417300, LR: 0.000500, Average Wealth: 26962.181640625, Average total costs paid: 7820.49658203125\n",
      "Epoch [800/3000], Loss: 0.0000417514, LR: 0.000500, Average Wealth: 26923.359375, Average total costs paid: 7827.02392578125\n",
      "Epoch [900/3000], Loss: 0.0000416011, LR: 0.000500, Average Wealth: 27081.228515625, Average total costs paid: 7856.9892578125\n",
      "Epoch [1000/3000], Loss: 0.0000418117, LR: 0.000250, Average Wealth: 26853.265625, Average total costs paid: 7824.6357421875\n",
      "Epoch [1100/3000], Loss: 0.0000415651, LR: 0.000250, Average Wealth: 27064.47265625, Average total costs paid: 7830.70068359375\n",
      "Epoch [1200/3000], Loss: 0.0000418661, LR: 0.000250, Average Wealth: 26876.185546875, Average total costs paid: 7827.89111328125\n",
      "Epoch [1300/3000], Loss: 0.0000416335, LR: 0.000250, Average Wealth: 27032.125, Average total costs paid: 7835.76953125\n",
      "Epoch [1400/3000], Loss: 0.0000420013, LR: 0.000250, Average Wealth: 26804.216796875, Average total costs paid: 7789.2236328125\n",
      "Epoch [1500/3000], Loss: 0.0000417483, LR: 0.000125, Average Wealth: 26932.30078125, Average total costs paid: 7836.64697265625\n",
      "Epoch [1600/3000], Loss: 0.0000417531, LR: 0.000125, Average Wealth: 26920.73828125, Average total costs paid: 7821.21435546875\n",
      "Epoch [1700/3000], Loss: 0.0000416174, LR: 0.000125, Average Wealth: 27090.009765625, Average total costs paid: 7857.140625\n",
      "Epoch [1800/3000], Loss: 0.0000416660, LR: 0.000125, Average Wealth: 26998.673828125, Average total costs paid: 7835.529296875\n",
      "Epoch [1900/3000], Loss: 0.0000419630, LR: 0.000125, Average Wealth: 26855.7109375, Average total costs paid: 7814.4453125\n",
      "Epoch [2000/3000], Loss: 0.0000417322, LR: 0.000063, Average Wealth: 26987.185546875, Average total costs paid: 7828.32080078125\n",
      "Epoch [2100/3000], Loss: 0.0000416856, LR: 0.000063, Average Wealth: 27002.216796875, Average total costs paid: 7834.18212890625\n",
      "Epoch [2200/3000], Loss: 0.0000416344, LR: 0.000063, Average Wealth: 26985.1484375, Average total costs paid: 7833.078125\n",
      "Epoch [2300/3000], Loss: 0.0000417026, LR: 0.000063, Average Wealth: 26907.224609375, Average total costs paid: 7826.4404296875\n",
      "Epoch [2400/3000], Loss: 0.0000418883, LR: 0.000063, Average Wealth: 26883.16796875, Average total costs paid: 7818.431640625\n",
      "Epoch [2500/3000], Loss: 0.0000415957, LR: 0.000031, Average Wealth: 27064.576171875, Average total costs paid: 7823.03173828125\n",
      "Epoch [2600/3000], Loss: 0.0000415363, LR: 0.000031, Average Wealth: 27151.82421875, Average total costs paid: 7859.4716796875\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 3000\n",
    "learning_rate = 0.001\n",
    "hidden_dim = 196\n",
    "input_dim = 1  # Asset price, time\n",
    "num_steps = 50  # Simulation steps\n",
    "num_paths = 20000  # Simulation paths\n",
    "x0, mu, sigma, lam = 13.95, 15.446, 0.606 * np.sqrt(252), 0.113 * 252\n",
    "ra = 2  # Risk aversion\n",
    "T = 1.0  # Time to maturity\n",
    "dt = T / num_steps\n",
    "# beta = 0.0001  # Transaction cost coefficient\n",
    "beta = 0.01\n",
    "r = 0.05  # Risk-free rate\n",
    "w0 = 10000  # Initial wealth\n",
    "h = dt\n",
    "var = sigma**2 * (1 - np.exp(-2 * lam * h)) / (2 * lam)\n",
    "\n",
    "# Ensure torch uses GPU\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "# Initialize the analytic solution\n",
    "analytic_model = analytic_solution(ra, r, lam, mu, sigma)\n",
    "\n",
    "# Initialize models, optimizers\n",
    "upper_net = BoundaryNetwork(input_dim, hidden_dim).to('cuda')\n",
    "lower_net = BoundaryNetwork(input_dim, hidden_dim).to('cuda')\n",
    "optimizer = optim.Adam(list(upper_net.parameters()) + list(lower_net.parameters()), lr=learning_rate)\n",
    "# Define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "\n",
    "# plotting data\n",
    "average_wealths = []\n",
    "losses = []\n",
    "fig,ax = plt.subplots()\n",
    "ax2 = ax.twinx()\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Average Wealth', color='b')\n",
    "ax2.set_ylabel('Loss', color='r')\n",
    "final_wealths = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Simulate price paths\n",
    "    wealth = torch.ones((num_paths, 1), device='cuda') * w0\n",
    "    x = torch.ones((num_paths, 1), device='cuda') * x0\n",
    "    pi_minus = torch.zeros_like(wealth, device='cuda')  # Starting with no position\n",
    "    y = torch.ones((num_paths, 1), device='cuda') * w0  # All wealth in riskless\n",
    "    total_costs = torch.zeros_like(wealth, device='cuda')  # Initialize total costs\n",
    "    t = 0\n",
    "    for _ in range(num_steps):\n",
    "        x = x.to(torch.float32)\n",
    "        wealth = wealth.to(torch.float32)\n",
    "        pi_minus = pi_minus.to(torch.float32)\n",
    "        # inputs = torch.cat([x, wealth, pi_minus], dim=1)\n",
    "        inputs = torch.cat([x], dim=1)\n",
    "\n",
    "        no_cost_position = torch.minimum(torch.maximum(analytic_model.optimal_nocost(t * dt, x), torch.tensor(-1.0, device='cuda')), torch.tensor(1.0, device='cuda'))\n",
    "\n",
    "        # Predict boundaries\n",
    "        upper_boundary = no_cost_position + upper_net(inputs)\n",
    "        upper_boundary = torch.maximum(torch.tensor(-1.0, device='cuda'), torch.minimum(upper_boundary, torch.tensor(1.0, device='cuda')))\n",
    "        lower_boundary = no_cost_position - lower_net(inputs)\n",
    "        lower_boundary = torch.minimum(torch.maximum(lower_boundary, torch.tensor(-1.0, device='cuda')), torch.tensor(1.0, device='cuda'))\n",
    "        pi_plus = torch.min(upper_boundary, torch.max(lower_boundary, pi_minus))\n",
    "        # pi_plus = torch.zeros_like(wealth, device='cuda')\n",
    "        t = t + h\n",
    "        # if (_ == num_steps/2) and epoch %100==0:\n",
    "        #   print(f'time: {_}')\n",
    "        #   print('pi_start', no_cost_position)\n",
    "        #   print('pi_plus:', pi_plus)\n",
    "        #   print('upper net', upper_net(inputs))\n",
    "        #   print('lower net', lower_net(inputs))\n",
    "\n",
    "        # Simulate OU process\n",
    "        dx = mu - mu * np.exp(-lam * h) + np.exp(-lam * h) * x - x + torch.normal(0, torch.sqrt(torch.tensor(var, device='cuda')), (num_paths, 1))\n",
    "        dy = (np.exp(r * h) - 1)\n",
    "        # c = beta * (wealth * (torch.abs(pi_plus - pi_minus)))**(3 / 2)\n",
    "        c = beta * (wealth * (torch.abs(pi_plus - pi_minus)))\n",
    "        total_costs = total_costs + c\n",
    "        dw = (pi_plus * wealth / x) * dx + (1 - pi_plus) * wealth * dy - c\n",
    "\n",
    "        wealth = wealth + dw\n",
    "        x = x + dx\n",
    "        y = y * (1 + dy)\n",
    "        pi_minus = (wealth - y) / wealth\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(wealth, w0, x0, T, analytic_model)\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    average_wealths.append(torch.mean(wealth).item())\n",
    "    final_wealths.extend(wealth.tolist())\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Step the scheduler for learning rate decay\n",
    "    scheduler.step()\n",
    "    # Print progress\n",
    "    if epoch % 100 == 0:\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(pi_plus)\n",
    "        print((f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.10f}, LR: {current_lr:.6f}, Average Wealth: {torch.mean(wealth).item()}, Average total costs paid: {torch.mean(total_costs).item()}'))\n",
    "\n",
    "print('Training finished.')\n",
    "\n",
    "ax.plot(average_wealths, color='b')\n",
    "ax2.plot(losses, color='r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a4527-e7fa-4d73-bfd7-9ec6064d024c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env [~/.conda/envs/torch-env/]",
   "language": "python",
   "name": "conda_torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
